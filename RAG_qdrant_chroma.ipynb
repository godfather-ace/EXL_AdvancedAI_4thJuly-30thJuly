{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index\n",
    "%pip install llama-index-vector-stores-qdrant \n",
    "%pip install llama-index-vector-stores-chroma\n",
    "%pip install llama-index chromadb\n",
    "%pip install llama-index-readers-file \n",
    "%pip install llama-index-llms-openai \n",
    "%pip install qdrant_client\n",
    "%pip install chromadb\n",
    "%pip install llama-index-embeddings-fastembed\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qdrant DB is a high-performance vector database designed for storing, indexing, and managing vector embeddings, similar to Chroma DB. It is specifically optimized for tasks involving similarity search, nearest neighbor search, and other operations on high-dimensional data. Qdrant DB is commonly used in machine learning and artificial intelligence applications where vector embeddings are generated and need to be efficiently queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "import qdrant_client\n",
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url = \"https://3a965baf-c90d-4ab7-935f-9ae69c90688d.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
    "    api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"sample_data\").load_data()\n",
    "vector_store = QdrantVectorStore(client = client, collection_name = \"tran\")\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "index = VectorStoreIndex(documents, storage_context = storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The term \"Attention Mechanism\" in Transformers refers to a mechanism that allows the model to focus on different parts of the input sequence when generating the output sequence. It enables the model to capture dependencies between different positions in the input and output sequences without being limited by their distance.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What is meant by the term Attention Mechanism in Transformers?\")\n",
    "display(Markdown(f\"<b>{res}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma DB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma DB, or Chroma Database, is a modern vector database designed to store and manage vector embeddings generated by machine learning models. These embeddings are numerical representations of data such as text, images, or other complex structures, which capture semantic information in a high-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 438M/438M [00:33<00:00, 12.9MB/s] \n",
      "c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sachi\\AppData\\Local\\llama_index\\models--BAAI--bge-base-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.73MB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 794kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "documents = SimpleDirectoryReader(\"sample_data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>An attention mechanism in Transformers refers to a function that maps a query and a set of key-value pairs to an output. It allows the model to focus on different parts of the input sequence when generating an output, by assigning weights to the key-value pairs based on the similarity between the query and the keys. This mechanism enables the model to capture dependencies between different positions in the input sequence and is a crucial component in facilitating effective information flow and context understanding within the Transformer architecture.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What is meant by the term Attention Mechanism in Transformers?\")\n",
    "display(Markdown(f\"<b>{res}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
