{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJGisMO4GuB3jiJYHrBskp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c676b075e6ae42b782b98ff27598276e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ee3842fc28c4c7f8853e0f5d1a22cec","IPY_MODEL_7adec8c9b972482f94c75b34375ec8b3","IPY_MODEL_288aaadb492b4c30a7a540d03b043750"],"layout":"IPY_MODEL_28c9a19a62dc4b2b881854036b9370ec"}},"7ee3842fc28c4c7f8853e0f5d1a22cec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87600f11913d40c58b5235d080591574","placeholder":"​","style":"IPY_MODEL_9f52f172badc4260b17bbc5f7889e4ac","value":"run_evals "}},"7adec8c9b972482f94c75b34375ec8b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea92c627e10f43cfbe53162e9cec8b82","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d9593cc6809415da74732240657ba0e","value":4}},"288aaadb492b4c30a7a540d03b043750":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2eb45277a104a3994c7482e3f6c574f","placeholder":"​","style":"IPY_MODEL_fcf493ea56a245de9363a2cc331eefe2","value":" 4/4 (100.0%) | ⏳ 00:06&lt;00:00 |  1.72s/it"}},"28c9a19a62dc4b2b881854036b9370ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87600f11913d40c58b5235d080591574":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f52f172badc4260b17bbc5f7889e4ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea92c627e10f43cfbe53162e9cec8b82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d9593cc6809415da74732240657ba0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2eb45277a104a3994c7482e3f6c574f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf493ea56a245de9363a2cc331eefe2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79f23cba1e6c407c84e1c62cee0051b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b90e805d0c94d93b35a718ebda60d54","IPY_MODEL_5de3bc3ca2de4533922c2804a1575cbb","IPY_MODEL_dba0660088074c81b3caf7727578f621"],"layout":"IPY_MODEL_289cbed872444baca67fef36fd7a541d"}},"0b90e805d0c94d93b35a718ebda60d54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07385e2d1562418380aebfdd0d2960ab","placeholder":"​","style":"IPY_MODEL_50d23408f5504c01899adc51664e10c0","value":"run_evals "}},"5de3bc3ca2de4533922c2804a1575cbb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ad019f993cc405cbbb3a7072e7381fd","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_407d55f5c2db4f97a17bdc69c1c8b074","value":4}},"dba0660088074c81b3caf7727578f621":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c8ca369f5094f939383ae6b75a2f4e2","placeholder":"​","style":"IPY_MODEL_6799c9bda73c4ba8972b4ad25c8d0ebe","value":" 4/4 (100.0%) | ⏳ 00:07&lt;00:00 |  1.46s/it"}},"289cbed872444baca67fef36fd7a541d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07385e2d1562418380aebfdd0d2960ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50d23408f5504c01899adc51664e10c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ad019f993cc405cbbb3a7072e7381fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"407d55f5c2db4f97a17bdc69c1c8b074":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c8ca369f5094f939383ae6b75a2f4e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6799c9bda73c4ba8972b4ad25c8d0ebe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"id":"RdNRYwQefFGv","executionInfo":{"status":"ok","timestamp":1720689627770,"user_tz":-330,"elapsed":11697,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"83f19b1a-048c-431c-92ea-e850a453c786"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: arize-phoenix[experimental,llama-index] in /usr/local/lib/python3.10/dist-packages (4.9.0)\n","Requirement already satisfied: openai>=1 in /usr/local/lib/python3.10/dist-packages (1.35.13)\n","Requirement already satisfied: getpass4 in /usr/local/lib/python3.10/dist-packages (0.0.14.1)\n","Collecting llama-index-callbacks-arize-phoenix\n","  Downloading llama_index_callbacks_arize_phoenix-0.1.6-py3-none-any.whl (2.2 kB)\n","Requirement already satisfied: aioitertools in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.11.0)\n","Requirement already satisfied: aiosqlite in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.20.0)\n","Requirement already satisfied: alembic<2,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.13.2)\n","Requirement already satisfied: arize-phoenix-evals>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.13.2)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (5.3.3)\n","Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.64.1)\n","Requirement already satisfied: hdbscan>=0.8.33 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.8.37)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.27.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (3.1.4)\n","Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.25.2)\n","Requirement already satisfied: openinference-instrumentation in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.1.8)\n","Requirement already satisfied: openinference-instrumentation-langchain>=0.1.12 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.1.21)\n","Requirement already satisfied: openinference-instrumentation-llama-index>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (2.0.0)\n","Requirement already satisfied: openinference-instrumentation-openai>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.1.7)\n","Requirement already satisfied: openinference-semantic-conventions>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.1.9)\n","Requirement already satisfied: opentelemetry-exporter-otlp in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.25.0)\n","Requirement already satisfied: opentelemetry-proto>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.25.0)\n","Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.25.0)\n","Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.46b0)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (2.0.3)\n","Requirement already satisfied: protobuf<6.0,>=3.20 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (3.20.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (5.9.5)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (14.0.2)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.0.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (6.0.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.11.4)\n","Requirement already satisfied: sqlalchemy[asyncio]<3,>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (2.0.31)\n","Requirement already satisfied: sqlean-py>=3.45.1 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (3.45.1)\n","Requirement already satisfied: starlette in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.37.2)\n","Requirement already satisfied: strawberry-graphql==0.235.0 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.235.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (4.66.4)\n","Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (4.12.2)\n","Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.5.6)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.30.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (1.14.1)\n","Requirement already satisfied: llama-index-agent-openai==0.2.7 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.2.7)\n","Requirement already satisfied: llama-index-embeddings-openai==0.1.10 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.1.10)\n","Requirement already satisfied: llama-index-llms-openai==0.1.24 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.1.24)\n","Requirement already satisfied: llama-index-readers-file==0.1.25 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.1.25)\n","Requirement already satisfied: llama-index==0.10.51 in /usr/local/lib/python3.10/dist-packages (from arize-phoenix[experimental,llama-index]) (0.10.51)\n","Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.1.12)\n","Requirement already satisfied: llama-index-core==0.10.51 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.10.51)\n","Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.2.4)\n","Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.9.48)\n","Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.1.7)\n","Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.1.6)\n","Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.1.3)\n","Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.1.6)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file==0.1.25->arize-phoenix[experimental,llama-index]) (4.12.3)\n","Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file==0.1.25->arize-phoenix[experimental,llama-index]) (4.2.0)\n","Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file==0.1.25->arize-phoenix[experimental,llama-index]) (0.0.26)\n","Requirement already satisfied: graphql-core<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.235.0->arize-phoenix[experimental,llama-index]) (3.2.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.235.0->arize-phoenix[experimental,llama-index]) (2.8.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (3.9.5)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (1.2.14)\n","Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (1.0.8)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (2023.6.0)\n","Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.0.6)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (3.3)\n","Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (3.8.1)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (9.4.0)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (2.31.0)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (8.4.2)\n","Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.7.0)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.9.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1) (1.7.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1) (2.8.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1) (1.3.1)\n","Requirement already satisfied: caugetch in /usr/local/lib/python3.10/dist-packages (from getpass4) (0.0.1)\n","Requirement already satisfied: clipboard in /usr/local/lib/python3.10/dist-packages (from getpass4) (0.0.4)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from getpass4) (0.4.6)\n","Requirement already satisfied: pyperclip in /usr/local/lib/python3.10/dist-packages (from getpass4) (1.9.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic<2,>=1.3.0->arize-phoenix[experimental,llama-index]) (1.3.5)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1) (1.2.1)\n","Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.33->arize-phoenix[experimental,llama-index]) (0.29.37)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.33->arize-phoenix[experimental,llama-index]) (1.4.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix[experimental,llama-index]) (2024.6.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->arize-phoenix[experimental,llama-index]) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->arize-phoenix[experimental,llama-index]) (0.14.0)\n","Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-langchain>=0.1.12->arize-phoenix[experimental,llama-index]) (1.25.0)\n","Requirement already satisfied: opentelemetry-instrumentation in /usr/local/lib/python3.10/dist-packages (from openinference-instrumentation-langchain>=0.1.12->arize-phoenix[experimental,llama-index]) (0.46b0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->arize-phoenix[experimental,llama-index]) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->arize-phoenix[experimental,llama-index]) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1) (2.20.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->arize-phoenix[experimental,llama-index]) (3.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix[experimental,llama-index]) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->arize-phoenix[experimental,llama-index]) (2.1.5)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.25.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix[experimental,llama-index]) (1.25.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.25.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix[experimental,llama-index]) (1.25.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.25.0->opentelemetry-exporter-otlp->arize-phoenix[experimental,llama-index]) (1.63.2)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.25.0->opentelemetry-exporter-otlp->arize-phoenix[experimental,llama-index]) (1.25.0)\n","Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->openinference-instrumentation-langchain>=0.1.12->arize-phoenix[experimental,llama-index]) (7.1.0)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn->arize-phoenix[experimental,llama-index]) (0.58.1)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn->arize-phoenix[experimental,llama-index]) (0.5.13)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->arize-phoenix[experimental,llama-index]) (8.1.7)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file==0.1.25->arize-phoenix[experimental,llama-index]) (2.5)\n","Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (0.4.6)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn->arize-phoenix[experimental,llama-index]) (0.41.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.235.0->arize-phoenix[experimental,llama-index]) (1.16.0)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation->openinference-instrumentation-langchain>=0.1.12->arize-phoenix[experimental,llama-index]) (67.7.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (4.0.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api->openinference-instrumentation-langchain>=0.1.12->arize-phoenix[experimental,llama-index]) (3.19.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (2024.5.15)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (2.0.7)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (1.0.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (3.21.3)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.51->llama-index==0.10.51->arize-phoenix[experimental,llama-index]) (24.1)\n","Installing collected packages: llama-index-callbacks-arize-phoenix\n","Successfully installed llama-index-callbacks-arize-phoenix-0.1.6\n"]}],"source":["!pip install \"arize-phoenix[experimental,llama-index]\" \"openai>=1\" getpass4  llama-index-callbacks-arize-phoenix"]},{"cell_type":"code","source":["import os\n","import openai\n","import pandas as pd\n","import phoenix as px\n","from getpass import getpass\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, set_global_handler\n","from llama_index.llms.openai import OpenAI\n","from phoenix.evals import (\n","    OpenAIModel,\n","    run_evals,\n",")\n","from tqdm import tqdm"],"metadata":{"id":"b_rGPOV6nO5K","executionInfo":{"status":"ok","timestamp":1720689515984,"user_tz":-330,"elapsed":532,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["session = px.launch_app()"],"metadata":{"id":"utd2d6lafNRB","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1720689524126,"user_tz":-330,"elapsed":4557,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"11f4d908-b396-429a-9856-3bc22d0e5024"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["🌍 To view the Phoenix app in your browser, visit https://udxwftg8ds1-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n","📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"]}]},{"cell_type":"code","source":["if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n","    openai_api_key = getpass(\"Enter your OpenAI API key: \")\n","openai.api_key = openai_api_key\n","os.environ[\"OPENAI_API_KEY\"] = openai_api_key"],"metadata":{"id":"Recg43BdnKS4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720689597314,"user_tz":-330,"elapsed":11023,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"1c231432-ba89-4667-cbb7-5e40bc9a4773"},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n"]}]},{"cell_type":"code","source":["set_global_handler(\"arize_phoenix\")"],"metadata":{"id":"sjftjtdV57Kq","executionInfo":{"status":"ok","timestamp":1720689641157,"user_tz":-330,"elapsed":3385,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Loading the data from the directory using SimpleDirectoryReader and building a VectorStoreIndex\n","documents = SimpleDirectoryReader(\"data\").load_data()\n","vector_index = VectorStoreIndex.from_documents(documents)\n","\n","# Initialising a query engine\n","query_engine = vector_index.as_query_engine()"],"metadata":{"id":"bZEzblErnSFX","executionInfo":{"status":"ok","timestamp":1720689713644,"user_tz":-330,"elapsed":5458,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Passing our queries (prompts) based on the data we have loaded\n","queries = [\"What is meant by the term Attention Mechanism?\", \"What are decoder-only transformers?\"]\n","queries"],"metadata":{"id":"L3_vuf6anVa3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720689771705,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"0f45d99f-19a7-4514-c67c-fca5d2fda877"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['What is meant by the term Attention Mechanism?',\n"," 'What are decoder-only transformers?']"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["for query in tqdm(queries):\n","    query_engine.query(query)"],"metadata":{"id":"et6FkhICnaYW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720689777371,"user_tz":-330,"elapsed":3226,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"fa62b152-8a43-4748-be17-9c25171b8805"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:03<00:00,  1.52s/it]\n"]}]},{"cell_type":"code","source":["from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n","from phoenix.trace import DocumentEvaluations, SpanEvaluations\n","from phoenix.evals import (\n","    HallucinationEvaluator,\n","    OpenAIModel,\n","    QAEvaluator,\n","    RelevanceEvaluator,\n","    run_evals,\n",")"],"metadata":{"id":"eTlfSEMApPTE","executionInfo":{"status":"ok","timestamp":1720689895619,"user_tz":-330,"elapsed":491,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["queries_df = get_qa_with_reference(session)\n","queries_df"],"metadata":{"id":"Ad5-h0Yu7O2C","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1720689898875,"user_tz":-330,"elapsed":581,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"3145695f-5dcc-44bd-dc86-269794c98000"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                           input  \\\n","context.span_id                                                    \n","f9abe045d3b86c9f  What is meant by the term Attention Mechanism?   \n","15d50c98d8d93d63             What are decoder-only transformers?   \n","\n","                                                             output  \\\n","context.span_id                                                       \n","f9abe045d3b86c9f  The term \"Attention Mechanism\" refers to a com...   \n","15d50c98d8d93d63  Decoder-only transformers are models that cons...   \n","\n","                                                          reference  \n","context.span_id                                                      \n","f9abe045d3b86c9f  Attention Visualizations\\nInput-Input Layer5\\n...  \n","15d50c98d8d93d63  1 Introduction\\nRecurrent neural networks, lon...  "],"text/html":["\n","  <div id=\"df-54be6933-2363-48fb-a5a7-3ff5d350e778\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","      <th>reference</th>\n","    </tr>\n","    <tr>\n","      <th>context.span_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>f9abe045d3b86c9f</th>\n","      <td>What is meant by the term Attention Mechanism?</td>\n","      <td>The term \"Attention Mechanism\" refers to a com...</td>\n","      <td>Attention Visualizations\\nInput-Input Layer5\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>15d50c98d8d93d63</th>\n","      <td>What are decoder-only transformers?</td>\n","      <td>Decoder-only transformers are models that cons...</td>\n","      <td>1 Introduction\\nRecurrent neural networks, lon...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54be6933-2363-48fb-a5a7-3ff5d350e778')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-54be6933-2363-48fb-a5a7-3ff5d350e778 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-54be6933-2363-48fb-a5a7-3ff5d350e778');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bcc4c296-ef8f-4d63-9e14-0575bb553557\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bcc4c296-ef8f-4d63-9e14-0575bb553557')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bcc4c296-ef8f-4d63-9e14-0575bb553557 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_ecdbb7aa-576c-471e-bd64-c14a45c80f70\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('queries_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_ecdbb7aa-576c-471e-bd64-c14a45c80f70 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('queries_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"queries_df","summary":"{\n  \"name\": \"queries_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"context.span_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"15d50c98d8d93d63\",\n          \"f9abe045d3b86c9f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"What are decoder-only transformers?\",\n          \"What is meant by the term Attention Mechanism?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Decoder-only transformers are models that consist of only the decoder part of the encoder-decoder architecture. These models generate an output sequence based solely on the input sequence and do not have an encoder component to map input sequences to continuous representations.\",\n          \"The term \\\"Attention Mechanism\\\" refers to a component in neural network architectures that allows the model to focus on specific parts of the input sequence when processing information. It helps the model weigh the importance of different input elements by assigning attention scores, enabling it to selectively attend to relevant parts of the input during processing.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht\\u22121and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21and\\u03b1= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10\",\n          \"Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb \\u2018making\\u2019, completing the phrase \\u2018making...more difficult\\u2019. Attentions here shown only for\\nthe word \\u2018making\\u2019. Different colors represent different heads. Best viewed in color.\\n13\\n\\nProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["retrieved_documents_df = get_retrieved_documents(session)\n","retrieved_documents_df"],"metadata":{"id":"OUwernao7Z4o","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1720689899415,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"aab2b905-7e39-4a34-c700-6c8453295bd2"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    context.trace_id  \\\n","context.span_id  document_position                                     \n","28633eaebebaa156 0                  ed76ff616d4315e4579ab7469a4d7e37   \n","                 1                  ed76ff616d4315e4579ab7469a4d7e37   \n","4d4b75610b63eb10 0                  806e11e4889d41b7e553074afb2d03be   \n","                 1                  806e11e4889d41b7e553074afb2d03be   \n","\n","                                                                             input  \\\n","context.span_id  document_position                                                   \n","28633eaebebaa156 0                  What is meant by the term Attention Mechanism?   \n","                 1                  What is meant by the term Attention Mechanism?   \n","4d4b75610b63eb10 0                             What are decoder-only transformers?   \n","                 1                             What are decoder-only transformers?   \n","\n","                                                                            reference  \\\n","context.span_id  document_position                                                      \n","28633eaebebaa156 0                  Attention Visualizations\\nInput-Input Layer5\\n...   \n","                 1                  Provided proper attribution is provided, Googl...   \n","4d4b75610b63eb10 0                  1 Introduction\\nRecurrent neural networks, lon...   \n","                 1                  Table 4: The Transformer generalizes well to E...   \n","\n","                                    document_score  \n","context.span_id  document_position                  \n","28633eaebebaa156 0                        0.793238  \n","                 1                        0.787569  \n","4d4b75610b63eb10 0                        0.789966  \n","                 1                        0.783031  "],"text/html":["\n","  <div id=\"df-dd146928-a9a3-4d3b-af5e-996ad63d875a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>context.trace_id</th>\n","      <th>input</th>\n","      <th>reference</th>\n","      <th>document_score</th>\n","    </tr>\n","    <tr>\n","      <th>context.span_id</th>\n","      <th>document_position</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">28633eaebebaa156</th>\n","      <th>0</th>\n","      <td>ed76ff616d4315e4579ab7469a4d7e37</td>\n","      <td>What is meant by the term Attention Mechanism?</td>\n","      <td>Attention Visualizations\\nInput-Input Layer5\\n...</td>\n","      <td>0.793238</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ed76ff616d4315e4579ab7469a4d7e37</td>\n","      <td>What is meant by the term Attention Mechanism?</td>\n","      <td>Provided proper attribution is provided, Googl...</td>\n","      <td>0.787569</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">4d4b75610b63eb10</th>\n","      <th>0</th>\n","      <td>806e11e4889d41b7e553074afb2d03be</td>\n","      <td>What are decoder-only transformers?</td>\n","      <td>1 Introduction\\nRecurrent neural networks, lon...</td>\n","      <td>0.789966</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>806e11e4889d41b7e553074afb2d03be</td>\n","      <td>What are decoder-only transformers?</td>\n","      <td>Table 4: The Transformer generalizes well to E...</td>\n","      <td>0.783031</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd146928-a9a3-4d3b-af5e-996ad63d875a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-dd146928-a9a3-4d3b-af5e-996ad63d875a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dd146928-a9a3-4d3b-af5e-996ad63d875a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-06b335d5-b4aa-4163-a353-63af729c8606\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06b335d5-b4aa-4163-a353-63af729c8606')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-06b335d5-b4aa-4163-a353-63af729c8606 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_b30ed19c-b8fd-43c4-bd53-a3c5d0bc2355\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('retrieved_documents_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b30ed19c-b8fd-43c4-bd53-a3c5d0bc2355 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('retrieved_documents_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"retrieved_documents_df","summary":"{\n  \"name\": \"retrieved_documents_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"context.trace_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"806e11e4889d41b7e553074afb2d03be\",\n          \"ed76ff616d4315e4579ab7469a4d7e37\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"What are decoder-only transformers?\",\n          \"What is meant by the term Attention Mechanism?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani\\u2217\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\u2217\\nGoogle Brain\\nnoam@google.comNiki Parmar\\u2217\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\u2217\\nGoogle Research\\nusz@google.com\\nLlion Jones\\u2217\\nGoogle Research\\nllion@google.comAidan N. Gomez\\u2217 \\u2020\\nUniversity of Toronto\\naidan@cs.toronto.edu\\u0141ukasz Kaiser\\u2217\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\u2217 \\u2021\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\n          \"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21and\\u03b1= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004296141712817792,\n        \"min\": 0.7830308449043782,\n        \"max\": 0.7932382425158554,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.7875686491840368,\n          0.7830308449043782\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["eval_model = OpenAIModel(model_name = \"gpt-3.5-turbo\")\n","hallucination_evaluator = HallucinationEvaluator(eval_model)\n","qa_correctness_evaluator = QAEvaluator(eval_model)\n","relevance_evaluator = RelevanceEvaluator(eval_model)\n","\n","hallucination_eval_df, qa_correctness_eval_df = run_evals(\n","    dataframe=queries_df,\n","    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n","    provide_explanation=True,\n",")\n","relevance_eval_df = run_evals(\n","    dataframe=retrieved_documents_df,\n","    evaluators=[relevance_evaluator],\n","    provide_explanation=True,\n",")[0]\n","\n","px.log_evaluations(\n","    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n","    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",")\n","px.log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))"],"metadata":{"id":"b8-Ur7877eSV","colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["c676b075e6ae42b782b98ff27598276e","7ee3842fc28c4c7f8853e0f5d1a22cec","7adec8c9b972482f94c75b34375ec8b3","288aaadb492b4c30a7a540d03b043750","28c9a19a62dc4b2b881854036b9370ec","87600f11913d40c58b5235d080591574","9f52f172badc4260b17bbc5f7889e4ac","ea92c627e10f43cfbe53162e9cec8b82","3d9593cc6809415da74732240657ba0e","d2eb45277a104a3994c7482e3f6c574f","fcf493ea56a245de9363a2cc331eefe2","79f23cba1e6c407c84e1c62cee0051b6","0b90e805d0c94d93b35a718ebda60d54","5de3bc3ca2de4533922c2804a1575cbb","dba0660088074c81b3caf7727578f621","289cbed872444baca67fef36fd7a541d","07385e2d1562418380aebfdd0d2960ab","50d23408f5504c01899adc51664e10c0","5ad019f993cc405cbbb3a7072e7381fd","407d55f5c2db4f97a17bdc69c1c8b074","9c8ca369f5094f939383ae6b75a2f4e2","6799c9bda73c4ba8972b4ad25c8d0ebe"]},"executionInfo":{"status":"ok","timestamp":1720689944863,"user_tz":-330,"elapsed":13605,"user":{"displayName":"Sachin Tripathi","userId":"15791126366825337649"}},"outputId":"e9775749-3536-43da-fe85-e5acc993fcab"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["The `model_name` field is deprecated. Use `model` instead.                 This will be removed in a future release.\n"]},{"output_type":"stream","name":"stderr","text":["WARNI [phoenix.evals.executors] 🐌!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"]},{"output_type":"display_data","data":{"text/plain":["run_evals |          | 0/4 (0.0%) | ⏳ 00:00<? | ?it/s"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c676b075e6ae42b782b98ff27598276e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNI [phoenix.evals.executors] 🐌!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"]},{"output_type":"display_data","data":{"text/plain":["run_evals |          | 0/4 (0.0%) | ⏳ 00:00<? | ?it/s"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79f23cba1e6c407c84e1c62cee0051b6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNI [phoenix.session.evaluation] This `log_evaluations` function is deprecated and will be removed in a future release. Please use `px.Client().log_evaluations(*evaluations)` instead.\n","WARNI [phoenix.session.evaluation] This `log_evaluations` function is deprecated and will be removed in a future release. Please use `px.Client().log_evaluations(*evaluations)` instead.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rGkkfiBBHKYc"},"execution_count":null,"outputs":[]}]}