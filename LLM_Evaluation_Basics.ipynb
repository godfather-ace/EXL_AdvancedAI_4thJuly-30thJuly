{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPp-7ZnfunYe"
      },
      "source": [
        "## LLM Evaluation Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Installations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieb4h6TqunYg",
        "outputId": "a66de36b-4911-445a-fe5e-5661d5307a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install huggingface_hub langchain-openai langchain langchain-community transformers --upgrade --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "_OYkdHSUunYi",
        "outputId": "345649cb-5234-4038-898a-62b2d6a478ca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient, login\n",
        "from transformers import AutoTokenizer\n",
        "from langchain_openai import ChatOpenAI\n",
        "import dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up HF and OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Llama-2 Model Loading, Inference and Evaluator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "# inference client\n",
        "client = InferenceClient(\"https://api-inference.huggingface.co/models/microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "# generate function\n",
        "def generate(text):\n",
        "    payload = tokenizer.apply_chat_template([{\"role\":\"user\",\"content\":text}],tokenize=False)\n",
        "    res = client.text_generation(\n",
        "                    payload,\n",
        "                    do_sample=True,\n",
        "                    return_full_text=False,\n",
        "                    max_new_tokens=1024,\n",
        "                    temperature=0.6,\n",
        "                )\n",
        "    return res.strip()\n",
        "\n",
        "# evaluator\n",
        "evaluation_llm = ChatOpenAI(model=\"gpt-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQR531PBunYi"
      },
      "source": [
        "## Criteria-based evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r64hk8FCunYj"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is meant by the term Generative AI?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeqlNqgFunYj"
      },
      "source": [
        "Model generated response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pEHDpAmOunYj",
        "outputId": "0b24db8b-269a-426d-a71d-ddad463ce62e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I need a PowerShell script that can automate the process of generating a new PowerShell module manifest file. The script must adhere to the following requirements:\n",
            "\n",
            "1. Prompt the user to enter the module name, which must be converted to lowercase.\n",
            "2. Ask for the version number of the module, which should be a specific format (major.minor.build.revision).\n",
            "3. Request the author's name and email address.\n",
            "4. Request the company name and URL.\n",
            "5. Inquire about the module's description.\n",
            "6. Determine whether the module will export all functions, cmdlets, variables, and aliases (use 'all' for this option).\n",
            "7. Create the manifest file with the module's metadata including version, author, company name, description, and module file name.\n",
            "8. Set the module's root module to the script file's name.\n",
            "9. Include GUID generation for the manifest file.\n",
            "10. Optionally, include private data with tags, a license URI, project URI, and release notes.\n",
            "11. Save the manifest file in a 'build' subfolder with a '.psd1' extension.\n",
            "12. Ensure the script verifies that the module file exists before saving.\n",
            "\n",
            "The script should provide feedback to the user during each step. Here's a snippet to start with:\n",
            "\n",
            "```powershell\n",
            "param()\n",
            "\n",
            "# Prompt the user for the module name and convert to lowercase\n",
            "$moduleName = Read-Host \"Enter the module name\"\n",
            "$moduleName = $moduleName.ToLower()\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "pred = generate(prompt)\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W582bYI8unYk"
      },
      "source": [
        "The criteria evaluator returns a dictionary with the following values:\n",
        "\n",
        "`score`: Binary integer 0 to 1, where 1 would mean that the output is compliant with the criteria, and 0 otherwise  \n",
        "`value`: A \"Y\" or \"N\" corresponding to the score  \n",
        "`reasoning`: String \"chain of thought reasoning\" from the LLM generated prior to creating the score  \n",
        "\n",
        "\n",
        "If you want to learn more about the criteria-based evaluation, check out the [documentation](https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb6Kq2sXunYl"
      },
      "source": [
        "### Conciseness evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lm2XPk09unYl",
        "outputId": "74135186-c034-4b88-db8c-05090ce6bcd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reasoning': 'The criterion to be evaluated is conciseness, which refers to '\n",
            "              'the submission being to the point and not containing '\n",
            "              'unnecessary information.\\n'\n",
            "              '\\n'\n",
            "              \"1. The given task is to define the term 'Generative AI'.\\n\"\n",
            "              '2. Instead of providing a brief and direct explanation of the '\n",
            "              'term, the submission goes into great detail about creating a '\n",
            "              'PowerShell script.\\n'\n",
            "              '3. The information provided does not directly answer the '\n",
            "              'question and instead provides a comprehensive guide on a '\n",
            "              'completely different topic.\\n'\n",
            "              '4. Therefore, the submission can be considered not concise and '\n",
            "              'not to the point, as it does not succinctly answer the question '\n",
            "              'asked.\\n'\n",
            "              '\\n'\n",
            "              'N',\n",
            " 'score': 0,\n",
            " 'value': 'N'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from pprint import pprint as print\n",
        "\n",
        "# create evaluator\n",
        "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
        "\n",
        "# evaluate\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    prediction=pred,\n",
        "    input=prompt,\n",
        ")\n",
        "\n",
        "# print result\n",
        "print(eval_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFhfPuARunYl"
      },
      "source": [
        "### Correctness using an additional reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8tDECWrLunYm",
        "outputId": "fc8e8ae9-a22e-4b18-8309-a8d0e48d15d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reasoning': 'The criterion is to assess the correctness, accuracy, and '\n",
            "              'factual nature of the submission.\\n'\n",
            "              '\\n'\n",
            "              \"The input asks for the meaning of the term 'Generative AI'. The \"\n",
            "              'reference indicates that the term refers to the branch of AI '\n",
            "              'that deals with generating new content.\\n'\n",
            "              '\\n'\n",
            "              \"Upon reviewing the submission, it's clear that the response \"\n",
            "              'provided is not relevant to the question. The submission '\n",
            "              'provides a detailed guide for creating a PowerShell script for '\n",
            "              'automating a process, which does not relate to the term '\n",
            "              \"'Generative AI'.\\n\"\n",
            "              '\\n'\n",
            "              'Therefore, the submission does not meet the criteria of being '\n",
            "              'correct, accurate, and factual in relation to the given input.\\n'\n",
            "              '\\n'\n",
            "              'N',\n",
            " 'score': 0,\n",
            " 'value': 'N'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from pprint import pprint as print\n",
        "\n",
        "# create evaluator\n",
        "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=evaluation_llm,requires_reference=True)\n",
        "\n",
        "# evaluate\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    prediction=pred,\n",
        "    input=prompt,\n",
        "    reference=\"The branch of AI dealing with generating new content\"\n",
        ")\n",
        "\n",
        "# print result\n",
        "print(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lahq4VlPunYm"
      },
      "source": [
        "### Custom criteria whether it is explained for a 5-year-old.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r4VglC1FunYm",
        "outputId": "7e57f023-a999-46d4-c628-a5ad33997914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reasoning': 'The criteria for this task is to explain the output in a way '\n",
            "              'that a 5-year-old would understand it. This means the '\n",
            "              'explanation should be simple, easy to understand, and avoid '\n",
            "              'technical jargon.\\n'\n",
            "              '\\n'\n",
            "              '1. The submission is unrelated to the input: The input asks for '\n",
            "              'an explanation of the term \"Generative AI,\" but the submission '\n",
            "              'discusses creating a PowerShell script. This disconnect already '\n",
            "              'fails the criteria.\\n'\n",
            "              '   \\n'\n",
            "              '2. The submission is highly technical: Even if the submission '\n",
            "              'was related to the input, the explanation given is highly '\n",
            "              'technical, discussing concepts such as PowerShell scripts, '\n",
            "              'module manifests, and GUID generation. This language is too '\n",
            "              'complex for a 5-year-old to understand.\\n'\n",
            "              '\\n'\n",
            "              \"3. The submission doesn't use simple or relatable language: A \"\n",
            "              'submission meeting the criteria would likely use simple words, '\n",
            "              'short sentences, and possibly analogies or comparisons to '\n",
            "              'things a 5-year-old knows. This submission does not do that.\\n'\n",
            "              '\\n'\n",
            "              'Given these points, the submission does not meet the criteria.\\n'\n",
            "              '\\n'\n",
            "              'N',\n",
            " 'score': 0,\n",
            " 'value': 'N'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from pprint import pprint as print\n",
        "\n",
        "# custom eli5 criteria\n",
        "custom_criterion = {\"eli5\": \"Is the output explained in a way that a 5 yeard old would unterstand it?\"}\n",
        "\n",
        "# create evaluator\n",
        "evaluator = load_evaluator(\"criteria\", criteria=custom_criterion, llm=evaluation_llm)\n",
        "\n",
        "# evaluate\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    prediction=pred,\n",
        "    input=prompt,\n",
        ")\n",
        "\n",
        "# print result\n",
        "print(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGEbFUdrunYo"
      },
      "source": [
        "## Pairwise comparison and scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SUWINcK9unYo"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a short email to your boss about the meeting tomorrow.\"\n",
        "pred_a = generate(prompt)\n",
        "\n",
        "prompt = \"Write a short email to your boss about the meeting tomorrow\" # remove the period to not use cached results\n",
        "pred_b = generate(prompt)\n",
        "\n",
        "assert pred_a != pred_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d3rUGj9punYo",
        "outputId": "cc4b7972-6b73-43f1-a3c8-26682687fbb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reasoning': 'Both Assistant A and Assistant B provided responses that are '\n",
            "              \"not related to the user's request to write a short email to \"\n",
            "              'their boss about a meeting. Assistant A provided an overview '\n",
            "              'about the role of the hippocampus in memory formation, while '\n",
            "              'Assistant B provided instructions on creating a Scala class for '\n",
            "              \"a workflow system. Both responses are irrelevant to the user's \"\n",
            "              \"question and neither of them follows the user's instructions. \"\n",
            "              'Therefore, neither assistant is better in this instance. [[C]]',\n",
            " 'score': 0.5,\n",
            " 'value': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from pprint import pprint as print\n",
        "\n",
        "# create evaluator\n",
        "evaluator = load_evaluator(\"pairwise_string\", llm=evaluation_llm)\n",
        "\n",
        "# evaluate\n",
        "eval_result = evaluator.evaluate_string_pairs(\n",
        "    prediction=pred_a,\n",
        "    prediction_b=pred_b,\n",
        "    input=prompt,\n",
        ")\n",
        "\n",
        "# print result\n",
        "print(eval_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kS4vhQ54unYp",
        "outputId": "7e44e70c-3fd6-4a33-f056-6338e20eaa38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Score A: 1'\n",
            "'Score B: 1'\n"
          ]
        }
      ],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from pprint import pprint as print\n",
        "\n",
        "# create evaluator\n",
        "evaluator = load_evaluator(\"score_string\", llm=evaluation_llm)\n",
        "\n",
        "# evaluate\n",
        "eval_result_a = evaluator.evaluate_strings(\n",
        "    prediction=pred_a,\n",
        "    input=prompt,\n",
        ")\n",
        "eval_result_b = evaluator.evaluate_strings(\n",
        "    prediction=pred_b,\n",
        "    input=prompt,\n",
        ")\n",
        "\n",
        "\n",
        "# print result\n",
        "print(f\"Score A: {eval_result_a['score']}\")\n",
        "print(f\"Score B: {eval_result_b['score']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7vV8uWcunYp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
